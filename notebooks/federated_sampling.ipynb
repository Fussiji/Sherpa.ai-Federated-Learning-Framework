{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emnist\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/f4/78b24acbef9e8fe976dda700f16a3606f3b8363b015bc555f8050fbbd8ac/emnist-0.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from emnist) (1.16.5)\n",
      "Requirement already satisfied: requests in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from emnist) (2.22.0)\n",
      "Requirement already satisfied: tqdm in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from emnist) (4.32.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from requests->emnist) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from requests->emnist) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from requests->emnist) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from requests->emnist) (2019.11.28)\n",
      "Installing collected packages: emnist\n",
      "Successfully installed emnist-0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_base.emnist import Emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199999\n",
      "40000\n",
      "40000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = Emnist()\n",
    "train_data, train_labels, val_data, val_labels, test_data, test_labels = database.load_data()\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]\n",
    "print(type(train_data[0]))\n",
    "train_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from matplotlib) (1.16.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: six in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n",
      "Requirement already satisfied: keras in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from keras) (1.16.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from keras) (1.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: h5py in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: keras_applications>=1.0.6 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: keras_preprocessing>=1.0.5 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: tensorflow in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (2.0.0b0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.16.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (0.1.7)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.16.5)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.14.0a20190603)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.14.0.dev2019060501)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (0.33.4)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: h5py in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow) (0.15.5)\n",
      "Requirement already satisfied: torch in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (1.4.0)\n",
      "Collecting node\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/38/22fe161e7431e3dd626e668876937a53a69df53a25574647cc485b757bbd/node-0.9.24.tar.gz (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 2.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/miguelangel/anaconda3/lib/python3.6/site-packages (from node) (41.0.1)\n",
      "Collecting odict>=1.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/7a/f91973c9461557fd0936255bc15379a3f39e97176f435ee79b15115a40ac/odict-1.7.0.tar.gz\n",
      "Collecting plumber>1.4.99\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/55/cad009be81ca45a748a8f3e435171b0e43f276c04a5d5f871ca0b1603511/plumber-1.5.tar.gz (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 18.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zope.lifecycleevent\n",
      "  Downloading https://files.pythonhosted.org/packages/15/4a/b7a1a421442dc700403d5aa710a84431e088200e069bbd279698affffeb2/zope.lifecycleevent-4.3-py2.py3-none-any.whl\n",
      "Collecting zope.deprecation\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/26/b935bbf9d27e898b87d80e7873a0200cebf239253d0afe7a59f82fe90fff/zope.deprecation-4.4.0-py2.py3-none-any.whl\n",
      "Collecting zope.component\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/97/81ce1c228e8a7e98ef92e8ff211b091cf5a223d6d69cb599368798084042/zope.component-4.6.tar.gz (84kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 13.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zope.interface\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d6/3f42b90b663139bd3f95abcd7385223087991ce40030fda251d5bbdc98fa/zope.interface-4.7.1-cp36-cp36m-macosx_10_6_intel.whl (140kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 5.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zope.event\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/96/361edb421a077a4c208b4a5c212737d78ae03ce67fbbcd01621c49f332d1/zope.event-4.4-py2.py3-none-any.whl\n",
      "Collecting zope.deferredimport>=4.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/63/e3/05b02057b56cd9c59d848b67aff1cc701e1d2237055ebd0d0c1f44331186/zope.deferredimport-4.3.1-py2.py3-none-any.whl\n",
      "Collecting zope.hookable>=4.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/90/a4d9e112d5cd7bcc5c4df253b2f517848d44268ec8f0a2fa0cecc16d8e2e/zope.hookable-5.0.0-cp36-cp36m-macosx_10_6_intel.whl\n",
      "Collecting zope.proxy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/75/7ead243867bb4482487bcc21cc80c26b4c8b2ef0278d4c543bd818ad305c/zope.proxy-4.3.3-cp36-cp36m-macosx_10_6_intel.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 13.6MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: node, odict, plumber, zope.component\n",
      "  Building wheel for node (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for node: filename=node-0.9.24-cp36-none-any.whl size=71682 sha256=9af163ff514681111738aa83abebb2d30d819401b85556c7852a65088c05aa74\n",
      "  Stored in directory: /Users/miguelangel/Library/Caches/pip/wheels/50/37/4b/666dad09eb5bef9975c87a750b64278f5baa5e3eabc30c3b9f\n",
      "  Building wheel for odict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for odict: filename=odict-1.7.0-py2.py3-none-any.whl size=14418 sha256=73fd9bba3bf36850caffd25d1dfc5d9698a381761151c87d3b433040374cc4ef\n",
      "  Stored in directory: /Users/miguelangel/Library/Caches/pip/wheels/79/a2/9f/c7ff9b7630b00f5e2a85d3bf18b285ec1e6ed8151d820aac2f\n",
      "  Building wheel for plumber (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for plumber: filename=plumber-1.5-cp36-none-any.whl size=24102 sha256=684b35f964b49557e2e74e5925cccc168dd365109a950018b724d5c06023aa95\n",
      "  Stored in directory: /Users/miguelangel/Library/Caches/pip/wheels/cc/b6/f6/ba1a0d5e9b9ea02b37eeecaa149e306942a230858958fd6c82\n",
      "  Building wheel for zope.component (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zope.component: filename=zope.component-4.6-py2.py3-none-any.whl size=65690 sha256=f3139594f5600c2684893b7f6abb9864f65c42184de296ce5912e100bb2c4e25\n",
      "  Stored in directory: /Users/miguelangel/Library/Caches/pip/wheels/4c/dd/f4/43127a5d0b629216ff0d0cbf2561845fb61ad49f17bcf980e6\n",
      "Successfully built node odict plumber zope.component\n",
      "Installing collected packages: odict, plumber, zope.interface, zope.event, zope.lifecycleevent, zope.deprecation, zope.proxy, zope.deferredimport, zope.hookable, zope.component, node\n",
      "Successfully installed node-0.9.24 odict-1.7.0 plumber-1.5 zope.component-4.6 zope.deferredimport-4.3.1 zope.deprecation-4.4.0 zope.event-4.4 zope.hookable-5.0.0 zope.interface-4.7.1 zope.lifecycleevent-4.3 zope.proxy-4.3.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.I.D. Federated Sampling\n",
    "\n",
    "In the IID scenario, each node has independent and identically distributed access to all observations in the dataset.\n",
    "\n",
    "The only available two choices are:\n",
    "1. Number of instances per node.\n",
    "2. Sampling with or without replacement.\n",
    "\n",
    "## Number of instances per node in a i.i.d scenario\n",
    "\n",
    "The *weight* parameter indicates the deterministic distribution of the number of samples per node, as a ratio over the total number of observations in the dataset. For instance, *weights = [0.2, 0.3, 0.5]* means that the first node will be assigned a 20% of the total number of observations in the dataset, the second node a 30% and the third node a 50%. \n",
    "\n",
    "Note that the *weight* parameter must not necessarily sum up to one. For instance, *weights = [0.2, 0.3, 0.1]* means that the first node will be assigned a 20% of the total number of observations in the dataset, the second node a 30% and the third node a 10%. \n",
    "\n",
    "## Sampling with or without replacement\n",
    "\n",
    "The *sampling* parameter, which can have one of the following two values *'with_replacement'* or *'without_replacement'*, indicates if an observation assigned to a particular node is removed from the dataset pool and thereof, it will be assigned only once (*weight = 'with_replacement'*); or will be back to the dataset pool and thereof, it could be selected for a new asignation (*weight = 'without_replacement'*).\n",
    "\n",
    "## Combinations of the *weights* and *sampling* parameters\n",
    "\n",
    "### *sampling = 'without_replacement'* \n",
    "\n",
    "When *sampling = 'without_replacement'* the total number of samples assigned to the nodes **can not be** greater than the number of available observations in the dataset. This imposes the constraint on the *weights* parameter than the sum of the *weights* values should be equal or lesser than one. The possible cases are:\n",
    "\n",
    "1. If the sum of the *weights* values is lesser than one when *sampling = 'without_replacement'* then, the resulting distribution of observations to the nodes (the union of the nodes sets of samples) is a subset of the raw dataset, such that the nodes do not share any observation among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1\n",
      "<class 'numpy.ndarray'>\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from data_distribution.data_distribution_iid import IidDataDistribution\n",
    "iid_distribution = IidDataDistribution(database)\n",
    "federated_train_data, federated_train_label, test_data, test_label = iid_distribution.get_federated_data(100)\n",
    "print(type(federated_train_data))\n",
    "print(federated_train_data.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. If the sum of the *weights* values is equal to one when *sampling = 'without_replacement'* then, the resulting distribution of observations to the nodes (the union of the nodes sets of samples) is eaxactly the raw dataset, that is, the distributed samples conform a partition of the original dataset.\n",
    "3. If the sum of the *weights* values is greater than one when *sampling = 'without_replacement'* then, the *weights* values will be normalized to sum up to one (Case 2). For instance, giving *sampling = 'without_replacement'* and *weights = [0.2, 0.3, 0.7]* the sum of the *weights* values is 1.2 > 1, and thereof, the effective *weights* values will result of the normalization: *weights = [0.2/1.2, 0.3/1.2, 0.7/1.2]*.\n",
    "\n",
    "### *sampling = 'with_replacement'* \n",
    "\n",
    "When *sampling = 'with_replacement'* the total number of samples assigned to the nodes **can be** greater than the number of available observations in the dataset. This removes any constraint on the *weights* parameter values. The resulting distribution of samples across the nodes are subsets of the original dataset that could share observations and also, each node could have none, one or more than one samples of a given observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_distribution.data_distribution_iid import IidDataDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_distribution = IidDataDistribution(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_data, federated_train_label, test_data, test_label = iid_distribution.get_federated_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2400, 28, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(federated_train_data))\n",
    "print(len(federated_train_data))\n",
    "federated_train_data[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "40000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40000, 28, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(test_data))\n",
    "print(len(test_data))\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 5, 6, 3, 2, 7, 8, 1, 9, 0], [7, 1, 5, 3, 8, 2, 4, 0, 9, 6], [6, 9, 8], [5, 3, 9, 6, 1, 2, 4], [3, 1, 0, 9, 2, 8, 5], [3, 0, 9, 6], [4, 0, 2, 1, 7], [1, 3, 0, 5, 9, 2, 8], [4, 8], [7, 0, 4, 6, 2, 3, 5, 1, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "from data_distribution.data_distribution_non_iid import NonIidDataDistribution\n",
    "\n",
    "non_iid_distribution = NonIidDataDistribution(database)\n",
    "federated_train_data_non_iid, federated_train_label_non_iid, test_data_non_iid, test_label_non_iid = non_iid_distribution.get_federated_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23999,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(federated_train_label_non_iid))\n",
    "print(len(federated_train_label_non_iid))\n",
    "np.unique(federated_train_label_non_iid[1])\n",
    "federated_train_label_non_iid[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights = np.random.dirichlet(np.ones(10),size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.dirichlet(np.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02558145 0.02443245 0.01430515 0.16062604 0.18786215 0.07697066\n",
      " 0.25474433 0.01377452 0.13411573 0.10758752]\n"
     ]
    }
   ],
   "source": [
    "np.sum(weights)\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_data, federated_train_label, test_data, test_label = iid_distribution.get_federated_data(10, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "10\n",
      "(32696, 28, 28)\n",
      "(1178, 28, 28)\n",
      "(388, 28, 28)\n",
      "(20858, 28, 28)\n",
      "(23304, 28, 28)\n",
      "(5151, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(type(federated_train_data))\n",
    "print(len(federated_train_data))\n",
    "print(federated_train_data[0].shape)\n",
    "print(federated_train_data[1].shape)\n",
    "print(federated_train_data[2].shape)\n",
    "print(federated_train_data[3].shape)\n",
    "print(federated_train_data[4].shape)\n",
    "print(federated_train_data[5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 3, 2, 8, 9], [7, 2, 6, 3, 5, 0, 8, 9, 4, 1], [6, 0, 2, 1, 5, 9, 4, 3, 8], [6, 0], [6, 3, 5, 0, 7, 1, 9], [6, 2, 5, 0, 8], [7, 3, 8, 2, 6], [6, 0, 5, 9, 3, 4, 2], [9, 3], [5, 1, 9, 4, 2, 6, 8, 3, 0]]\n"
     ]
    }
   ],
   "source": [
    "federated_train_data_non_iid, federated_train_label_non_iid, test_data_non_iid, test_label_non_iid = non_iid_distribution.get_federated_data(10, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13524536 0.0195398  0.09648692 0.01291804 0.05955207 0.10363137\n",
      " 0.24989205 0.05102651 0.03263156 0.23907632]\n",
      "<class 'numpy.ndarray'>\n",
      "10\n",
      "(16229, 28, 28)\n",
      "(1875, 28, 28)\n",
      "(16209, 28, 28)\n",
      "(2170, 28, 28)\n",
      "(5716, 28, 28)\n",
      "(19897, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(weights)\n",
    "print(type(federated_train_data_non_iid))\n",
    "print(len(federated_train_data_non_iid))\n",
    "print(federated_train_data_non_iid[0].shape)\n",
    "print(federated_train_data_non_iid[1].shape)\n",
    "print(federated_train_data_non_iid[2].shape)\n",
    "print(federated_train_data_non_iid[3].shape)\n",
    "print(federated_train_data_non_iid[4].shape)\n",
    "print(federated_train_data_non_iid[5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
    "from model.model import TrainableModel\n",
    "\n",
    "\n",
    "class DeepLearningModel(TrainableModel):\n",
    "\n",
    "    def __init__(self, model, batch_size, epochs):\n",
    "        self._model = Sequential()\n",
    "\n",
    "        self._model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', strides=1, input_shape=(28, 28, 1)))\n",
    "        self._model.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\n",
    "        self._model.add(Dropout(0.4))\n",
    "        self._model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', strides=1))\n",
    "        self._model.add(MaxPooling2D(pool_size=2, strides=2, padding='valid'))\n",
    "        self._model.add(Dropout(0.7))\n",
    "        self._model.add(Flatten())\n",
    "        self._model.add(Dense(128, activation='relu'))\n",
    "        self._model.add(Dropout(0.1))\n",
    "        self._model.add(Dense(64, activation='relu'))\n",
    "        self._model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._epochs = epochs\n",
    "\n",
    "    def train(self, data, labels):\n",
    "        if len(data.shape) == 3:\n",
    "            data = np.reshape(data, (data.shape[0], data.shape[1], data.shape[2],1))\n",
    "\n",
    "        if len(labels.shape) == 1:\n",
    "            labels = keras.utils.to_categorical(labels, 10)\n",
    "\n",
    "        # Save best model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "        save = ModelCheckpoint('best_model.hdf5', save_best_only=True, monitor='val_loss', mode='auto', verbose=0)\n",
    "\n",
    "        self._model.fit(x=data, y=labels, batch_size=self._batch_size, epochs=self._epochs, validation_split=0.2,\n",
    "                        verbose=0, shuffle=False, callbacks=[early_stopping, save])\n",
    "\n",
    "    def predict(self, data, batch_size = None):\n",
    "        data = np.reshape(data, (data.shape[0], data.shape[1], data.shape[2], 1))\n",
    "        return self._model.predict(data, batch_size = batch_size).argmax(axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
