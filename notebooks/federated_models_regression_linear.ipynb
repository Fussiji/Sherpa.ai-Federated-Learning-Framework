{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning: linear regression for a 2D simple case\n",
    "\n",
    "The present notebook tackles the problem of assessing the accuracy of a specific model into a federated configuration. \n",
    "In this case, we will create a learning model from scratch and show how to make it interact with the Federated Learning platform. \n",
    "In particular, we assess the *accuracy* in a federated learning context, and we address the *privacy level needed* in terms of sampling the sensitivity of our model.\n",
    "For a more sophisticated yet more realistic regression example, see the notebook on [Regression using Keras](./federated_models_regression_deeplearning.ipynb), where the California Housing dataset is used. \n",
    "\n",
    "## Model definition\n",
    "In order to make our model interact with the Federated Learning platform we will simply need to define:\n",
    "1. How to load the data;\n",
    "2. The model.\n",
    "\n",
    "In the following, each step is described for the case of a 2D linear regression model. \n",
    "\n",
    "**How to load the data**\\\n",
    "A method that returns train, test and validation data need to be provided, wrapping it in the class `data_base`.\n",
    "Typically, existing data is used. \n",
    "However, in this example a series of 2D points is created for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shfl\n",
    "from shfl.data_base import data_base as db\n",
    "\n",
    "# Create database: \n",
    "class LinearRegressionDB(db.DataBase):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._beta0 = 10    # True linear coefficients\n",
    "        self._beta1 = 2     # True linear coefficients\n",
    "        self._scale = 10    # Noise added to the true linear regression\n",
    "        self._size_train = 80\n",
    "        self._size_test = 20\n",
    "        self._size_validation = 20\n",
    "        \n",
    "    def _create_data(self, size_data):\n",
    "        data = np.random.randint(low = 0, high=100, size=size_data, dtype='l')\n",
    "        labels = self._beta0 + self._beta1*data + np.random.normal(loc=0.0, scale=self._scale, size=len(data))\n",
    "        \n",
    "        return(data, labels) \n",
    "       \n",
    "    def load_data(self):\n",
    "        self._train_data, self._train_labels = self._create_data(self._size_train)\n",
    "        self._test_data, self._test_labels = self._create_data(self._size_test)\n",
    "        self._validation_data, self._validation_labels = self._create_data(self._size_validation)\n",
    "        \n",
    "        return self.data\n",
    "\n",
    "database = LinearRegressionDB()\n",
    "train_data, train_labels, val_data, val_labels, test_data, test_labels = database.load_data()\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the model** <br/>\n",
    "Now we just need to define the model, which needs to be wrapped in the class `TrainableModel`. \n",
    "Abstract methods from class `TrainableModel` need to be defined, i.e. we must provide functions for `train`, `predict`, `evaluate`, `get_parameters` and `set_parameters`. \n",
    "In the case of 2D linear regression, a possible implementation is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.model import TrainableModel\n",
    "\n",
    "class LinearRegression2D(TrainableModel):\n",
    "    \n",
    "    def __init__(self, beta0 = 0.0, beta1=0.0):\n",
    "        self._beta0 = beta0\n",
    "        self._beta1 = beta1\n",
    "        \n",
    "    def train(self, data, labels):\n",
    "        \"\"\"\n",
    "        In the case of 2D linear regression, a closed formula can be used.\n",
    "        \"\"\"\n",
    "        data_mean = np.mean(data)\n",
    "        labels_mean = np.mean(labels)\n",
    "        beta1 = np.sum( np.multiply((data-data_mean), (labels-labels_mean)) ) / np.sum( np.square((data-data_mean)) )\n",
    "        beta0 = labels_mean - beta1*data_mean\n",
    "        self._beta0 = beta0\n",
    "        self._beta1 = beta1\n",
    "        \n",
    "    def predict(self, data):\n",
    "        y_predicted = self._beta0 + self._beta1 * data\n",
    "        \n",
    "        return(y_predicted)\n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        \"\"\"\n",
    "        Add here all the metrics to evaluate the performance. \n",
    "        \"\"\"\n",
    "        prediction = self.predict(data)\n",
    "        error = np.square(labels - prediction)\n",
    "        RMSE = np.sqrt(error.mean())\n",
    "        MAPE = np.abs(np.divide(error, labels)).mean() \n",
    "        \n",
    "        return RMSE, MAPE \n",
    "\n",
    "    def get_model_params(self):\n",
    "        return (self._beta0, self._beta1)\n",
    "    \n",
    "    def set_model_params(self, params):\n",
    "        self._beta0 = params[0]\n",
    "        self._beta1 = params[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can graphically check that our implementation is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the regression over the train data:\n",
    "LR = LinearRegression2D()\n",
    "LR.train(data = train_data, labels = train_labels)\n",
    "print(\"Regression coefficients: \" + str((LR._beta0, LR._beta1)))\n",
    "print(\"Performance metrics: \" + str(LR.evaluate(data = train_data, labels = train_labels)))\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "ax.plot(train_data, train_labels, 'bo', label = \"True\")\n",
    "ax.plot(train_data, LR.predict(data = train_data), label = \"Predicted\", color = \"red\")\n",
    "ax.set_xlabel('Data')\n",
    "ax.set_ylabel('Labels')\n",
    "plt.legend(title = \"\")\n",
    "label=\"Linear regression (red line) using using the training set (blue points)\"\n",
    "ax.text((train_data.max()+train_data.min())/2, -60, label, ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model in a Federated configuration\n",
    "After defining the data and the model, we are ready to run our model in a federated configuration. \n",
    "We distribute the data over the nodes, assuming the data is IID.\n",
    "Next, we define the aggregation of the federated outputs to be the average. \n",
    "In this case, we set the number of rounds `n=1` since no iterations are needed in this specific case of 2D linear regression. \n",
    "It can be observed that the performance of *Global model* is in general superior with respect to the performance of each node, thus the federated learning approach proves to be beneficial.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the IID data: \n",
    "iid_distribution = shfl.data_distribution.IidDataDistribution(database)\n",
    "federated_data, test_data, test_label = iid_distribution.get_federated_data(num_nodes = 12, percent=100)\n",
    "print(type(federated_data))\n",
    "print(federated_data.num_nodes())\n",
    "\n",
    "# Run the algorithm:\n",
    "aggregator = shfl.federated_aggregator.AvgFedAggregator()\n",
    "federated_government = shfl.learning_approach.FederatedGovernment(LinearRegression2D, federated_data, aggregator)\n",
    "\n",
    "federated_government.run_rounds(n = 1, test_data = test_data, test_label = test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy level needed: sample sensitivity\n",
    "In the case of applying the Laplace privacy mechanism (see also [Laplace mechanism notebook](./differential_privacy_laplace.ipynb)), the noise added has to be of the order of the sensitivity of the model's output (the values of the intercept and slope in our 2D linear regression). \n",
    "In the general case, model's sensitivity might be difficult to compute analytically. \n",
    "An alternative approach is to attain *random* differential privacy through a sampling over the data (see [Rubinstein 2017](https://arxiv.org/pdf/1706.02562.pdf)). That is, instead of computing analytically the *global* sensitivity $\\Delta f$, we compute an *empirical estimation* of it by sampling over the dataset.\n",
    "The framework provides a method to sample the sensitivity (see the implementation [here](../shfl/differential_privacy/sensitivity_sampler.py)).\n",
    "\n",
    "**Define a data distribution** \\\n",
    "In order to carry out this approach, we need to specify a distribution of the data to sample from. \n",
    "This in general requires previous knowledge and/or model assumptions. \n",
    "However, in our specific case of randomly generated data, we may assume that the data distribution is *uniform*. \n",
    "To the end, we define our class of `ProbabilityDistribution` that uniformly samples over a data-frame.\n",
    "Moreover, we assume that we do have access to a set of data (this can be thought, for example, as some reference public data set). \n",
    "In this example, we generate new data as in the beginning and use the validation set for sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformDistribution(shfl.differential_privacy.ProbabilityDistribution):\n",
    "    \"\"\"\n",
    "    Implement Uniform Distribution over real data\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_data):\n",
    "        self._sample_data = sample_data\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        row_indices = np.random.randint(low = 0, \n",
    "                                        high=self._sample_data.shape[0], \n",
    "                                        size=sample_size, \n",
    "                                        dtype='l')\n",
    "        \n",
    "        return self._sample_data[row_indices, :]\n",
    "    \n",
    "# Generate new data for sampling: \n",
    "database_sample = LinearRegressionDB()\n",
    "_, _, data_sample, labels_sample, _, _ = database.load_data()\n",
    "sample_data = np.zeros((len(data_sample), 2))\n",
    "sample_data[:,0] = data_sample\n",
    "sample_data[:,1] = labels_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling**\\\n",
    "The class `SensitivitySampler` implements the sampling given a *query* (which is the model itself in this case).\n",
    "We only need to add the method `get` to our model since it is required by the class. \n",
    "We choose the sensitivity norm to be the $L_1$ norm and we apply the sampling. \n",
    "The value of the sensitivity depends on the number of samples `n`: the more samples we perform, the more accurate the sensitivity. \n",
    "Indeed, increasing the number of samples `n`, the sensitivity decreases, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import SensitivitySampler\n",
    "from shfl.differential_privacy import L1SensitivityNorm\n",
    "\n",
    "class LinearRegression2DSample(LinearRegression2D):\n",
    "    \n",
    "    def get(self, data_array):\n",
    "        data = data_array[:, 0]\n",
    "        labels = data_array[:, 1]\n",
    "        train_model = self.train(data, labels)\n",
    "        \n",
    "        return np.asarray(self.get_model_params())\n",
    "\n",
    "distribution = UniformDistribution(sample_data)\n",
    "sampler = SensitivitySampler()\n",
    "\n",
    "n_samples = 5\n",
    "sensitivity, mean = sampler.sample_sensitivity(LinearRegression2DSample(), L1SensitivityNorm(), distribution, n=n_samples, gamma=0.05)\n",
    "print(\"Sampled max sensitivity: \" + str(sensitivity))\n",
    "print(\"Sampled mean sensitivity: \" + str(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 15\n",
    "sensitivity, mean = sampler.sample_sensitivity(LinearRegression2DSample(), L1SensitivityNorm(), distribution, n=n_samples, gamma=0.05)\n",
    "print(\"Sampled max sensitivity: \" + str(sensitivity))\n",
    "print(\"Sampled mean sensitivity: \" + str(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
