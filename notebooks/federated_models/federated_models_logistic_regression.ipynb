{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning: Classification by logistic regression\n",
    "\n",
    "We show how to set up a Federated classification experiment using a Logistic Regression model.\n",
    "Results from the Federated Learning are compared to the (non-federated) centralized learning.\n",
    "Moreover, we also show how the addition of Differential Privacy affects the performance of the Federated model. \n",
    "In the present examples, we will generate synthetic data for the classification task. \n",
    "In particular, we will start from a two-dimensional case, since with only two features we are able to easily plot the samples and the decision boundaries computed by the classifier. \n",
    "After that, we will repeat the experiment by adding more features and classes to the synthetic database.\n",
    "\n",
    "## Two-features case: \n",
    "We generate the data using the `make_classification` function from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shfl\n",
    "from shfl.data_base.data_base import LabeledDatabase\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from shfl.private.reproducibility import Reproducibility\n",
    "\n",
    "# Comment to turn off reproducibility:\n",
    "Reproducibility(1234)\n",
    "\n",
    "# Create database:\n",
    "n_features = 2\n",
    "n_classes = 3\n",
    "n_samples = 500\n",
    "data, labels = make_classification(\n",
    "    n_samples=n_samples, n_features=n_features, n_informative=2, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=1, weights=None, flip_y=0.1, class_sep=0.4, random_state=1234)\n",
    "database = LabeledDatabase(data, labels)\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = database.load_data()\n",
    "\n",
    "print(\"Shape of train and test data: \" + str(train_data.shape) + str(test_data.shape))\n",
    "print(\"Shape of train and test labels: \" + str(train_labels.shape) + str(test_labels.shape))\n",
    "print(train_data[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, in this two-features case it is beneficial to visualize the results. \n",
    "For that purpose, we will use the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2D_decision_boundary(model, data, labels, title=None):\n",
    "    # Step size of the mesh. Smaller it is, better the quality\n",
    "    h = .02 \n",
    "    # Color map\n",
    "    cmap = plt.cm.Set1\n",
    "    \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=cmap,\n",
    "               alpha=0.6,\n",
    "               aspect='auto', origin='lower')\n",
    "    # Plot data:\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=cmap, s=40, marker='o')\n",
    "    \n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('Feature 1', fontsize=18)\n",
    "    plt.ylabel('Feature 2', fontsize=18)\n",
    "    plt.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the Federated model.** \n",
    "After defining the data, we are ready to run our model in a federated configuration. \n",
    "We distribute the data over the nodes, assuming the data is IID.\n",
    "Next, we define the aggregation of the federated outputs to be the *average* of the clients' models. \n",
    "\n",
    "The *Sherpa.ai Federated Learning Framework* offers support for the Logistic Regression model from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). The user must specify in advance the number of features and the target classes: the assumption for this Federated Logistic Regression example is that each client's data possesses at least one sample of each class.\n",
    "Otherwise, each node might train a different classification problem, and it would be problematic to aggregate the global model.\n",
    "Setting model's state parameter to `warm_start:True` tells the clients to restart the training from the Federated round update.\n",
    "For assessing the performance, we compute the *Balanced Accuracy* and the *Kohen Kappa* scores (see [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.model.linear_classifier_model import LogisticRegressionModel\n",
    "\n",
    "# Distribute data over the nodes:\n",
    "iid_distribution = shfl.data_distribution.IidDataDistribution(database)\n",
    "n_clients = 5\n",
    "federated_data, test_data, test_labels = iid_distribution.get_federated_data(num_nodes=n_clients, percent=100)\n",
    "aggregator = shfl.federated_aggregator.FedAvgAggregator()\n",
    "\n",
    "# Define the model:\n",
    "classes = np.unique(train_labels)\n",
    "def model_builder():\n",
    "    model = LogisticRegressionModel(n_features=n_features, classes=classes, model_inputs={'warm_start':True})\n",
    "    return model\n",
    "\n",
    "# Run the Federated experiment:\n",
    "federated_government = shfl.federated_government.FederatedGovernment(model_builder, federated_data, aggregator)\n",
    "federated_government.run_rounds(n=2, test_data=test_data, test_label=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the performance of the *Federated Global model* is in general superior with respect to the performance of each node, thus the federated learning approach proves to be beneficial.\n",
    "Moreover, since no or little performance difference is observed between the federated rounds, we can conclude that the classification problem converges very early for this setting and no further rounds are required. This might be due to IID nature of clients' data when performing classification: each node gets a representative chunk of data and thus the local models are similar.\n",
    "\n",
    "**Comparison to centralized training.** The performance of *Federated Global model* is *comparable* to the performance of the model trained on centralized data, and it produces similar decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on centralized data for comparison:\n",
    "model_centralized = LogisticRegressionModel(n_features=n_features, classes=classes)\n",
    "model_centralized.train(train_data, train_labels)\n",
    "print('Centralized test performance: ' + str(model_centralized.evaluate(test_data, test_labels)))\n",
    "\n",
    "# Plot decision boundaries and test data for Federated and (non-Federated) centralized cases:\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(federated_government.global_model, test_data, test_labels, title = \"Federated Logistic regression. Test data are shown.\")\n",
    "    plot_2D_decision_boundary(model_centralized._model, test_data, test_labels, title = \"Centralized Logistic regression. Test data are shown.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Differential Privacy to the Federated model.** \n",
    "We want to assess the impact of Differential Privacy (see [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf), Section 3.3) on the Federated model's performance. \n",
    "In particular, we will use the Laplace mechanism (see also the corresponding [Laplace mechanism notebook](../differential_privacy/differential_privacy_laplace.ipynb)). \n",
    "The noise added has to be of the order of the sensitivity of the model's output, i.e. the model parameters of our logistic regression. \n",
    "In general, the sensitivity of a Machine Learning model is difficult to compute (for the Logistic Regression case, refer to [Privacy-preserving logistic regression](http://papers.nips.cc/paper/3486-privacy-preserving-logistic-regression.pdf)). \n",
    "An alternative strategy may be to estimate the sensitivity through a *sampling* procedure (e.g. see [Rubinstein 2017](https://arxiv.org/abs/1706.02562) and how to use the tools provided by Sherpa.ai Federated Learning Framework in the [Linear Regression Notebook](../federated_models/federated_models_linear_regression.ipynb)). \n",
    "However, be advised that this would guarantee the *weaker* property of *random* differential privacy.\n",
    "This approach is convenient since allows for the sensitivity estimation of an arbitrary model or a black-box computer function.\n",
    "The *Sherpa.ai Federated Learning Framework* provides this functionality in the class `SensitivitySampler`.\n",
    "\n",
    "\n",
    "We need to specify a distribution of the data to sample from, and this in general requires previous knowledge and/or model assumptions. \n",
    "In order not make any specific assumption on the distribution of the dataset, we can choose a *uniform* distribution. \n",
    "To the end, we define our class of `ProbabilityDistribution` that uniformly samples over a data-frame.\n",
    "We could sample using the train data. However, since in a real case the train data would be the actual clients' data, we wouldn't have access to it. \n",
    "Thus we generate another synthetic dataset for sampling (in the real case, this could be some public database we can access to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformDistribution(shfl.differential_privacy.ProbabilityDistribution):\n",
    "    \"\"\"\n",
    "    Implement Uniform sampling over the data\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_data):\n",
    "        self._sample_data = sample_data\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        row_indices = np.random.randint(low=0, high=self._sample_data.shape[0], size=sample_size, dtype='l')\n",
    "        \n",
    "        return self._sample_data[row_indices, :]\n",
    "    \n",
    "# Create sampling database:\n",
    "n_samples = 150\n",
    "sampling_data, sampling_labels = make_classification(\n",
    "    n_samples=n_samples, n_features=n_features, n_informative=2, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=1, weights=None, flip_y=0.1, class_sep=0.1)    \n",
    "sample_data = np.hstack((sampling_data, sampling_labels.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `SensitivitySampler` implements the sampling given a *query*, i.e. the learning model itself in this case.\n",
    "We only need to add the method `get` to our model since it is required by the class `SensitivitySampler`: it simply trains the model on the input data and outputs the trained parameters. \n",
    "We choose the sensitivity norm to be the $L_1$ norm and we apply the sampling. \n",
    "The value of the sensitivity depends on the number of samples `n`: the more samples we perform, the more accurate the sensitivity. \n",
    "Indeed, increasing the number of samples `n`, the sensitivity gets more accurate and typically decreases.\n",
    "Note that the sampling could be quite costly, since at each time, the query (i.e. the model, in this case) is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import SensitivitySampler\n",
    "from shfl.differential_privacy import L1SensitivityNorm\n",
    "\n",
    "class LogisticRegressionSample(LogisticRegressionModel):\n",
    "    \n",
    "    def get(self, data_array):\n",
    "        data = data_array[:, 0:-1]\n",
    "        labels = data_array[:, -1]\n",
    "        train_model = self.train(data, labels)\n",
    "      \n",
    "        return self.get_model_params()\n",
    "\n",
    "distribution = UniformDistribution(sample_data)\n",
    "sampler = SensitivitySampler()\n",
    "\n",
    "n_samples = 200\n",
    "max_sensitivity, mean_sensitivity = sampler.sample_sensitivity(\n",
    "    LogisticRegressionSample(n_features=n_features, classes=classes), \n",
    "    L1SensitivityNorm(), distribution, n=n_samples, gamma=0.05)\n",
    "print(\"Max sensitivity from sampling: \" + str(max_sensitivity))\n",
    "print(\"Mean sensitivity from sampling: \" + str(mean_sensitivity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once obtained the model's estimated sensitivity, we fix the $\\epsilon$ privacy budget and we can run the privacy-preserving Federated experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import LaplaceMechanism\n",
    "\n",
    "params_access_definition = LaplaceMechanism(sensitivity=max_sensitivity, epsilon=0.5)\n",
    "federated_governmentDP = shfl.federated_government.FederatedGovernment(\n",
    "    model_builder, federated_data, aggregator, model_params_access=params_access_definition)\n",
    "\n",
    "federated_governmentDP.run_rounds(n=2, test_data=test_data, test_label=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one could expect, the addition of random noise slightly alters the solution, but the model is still comparable to the non-private federated case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries and test data for Privacy-preserving Federated case:\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(federated_governmentDP.global_model, test_data, test_labels,\n",
    "                              title = \"Privacy-preserving Federated Logistic regression. Test data are shown.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark 1:** In this case we only run a few Federated rounds. However, in general one should make sure not to exceed a fixed privacy budget (see how to achieve that  in *Sherpa.ai Federated Learning Framework* in the [Composition concepts notebook](../differential_privacy/differential_privacy_composition_concepts.ipynb)).   \n",
    "**Remark 2:** It is worth mentioning that the above results cannot be considered as general. Some factors that considerably influence the classification problem are, for example, the training dataset, the model type, and the Differential Privacy mechanism used. \n",
    "In fact, the classification problem itself depends on the training data (number of features, whether the classes are separable or not etc.). \n",
    "We strongly encourage the user to play with the values for generating the database (`n_features, n_classes, n_samples, class_sep ...` see [their meaning](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification)), or try different classification datasets, since convergence and accuracy of local and global models can be strongly affected. \n",
    "Moreover, even without changing the dataset, by running the present experiment multiple times (you need to comment the *Reproducibility* command), it is observed that the *Federated Global model* may also exhibit slightly *better performance* when compared to the centralized model (we use `random_state` input for producing always the same dataset). \n",
    "Similarly, the privacy-preserving Federate model might exhibit even better performance compared to the non-private version. \n",
    "This depends on a) the performance metrics chosen and b) the idiosyncrasy of the specific classification considered here: a small modification to the model's coefficients may alter the class prediction for a few samples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case with more features and classes: \n",
    "Below we present a more complex case, introducing more features and classes. When using more than 2 features, the figures are not plotted. Since the structure of the example is identical as above, the comments are not repeated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database:\n",
    "n_features = 11\n",
    "n_classes = 5\n",
    "n_samples = 500\n",
    "data, labels = make_classification(\n",
    "    n_samples=n_samples, n_features=n_features, n_informative=4, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=2, weights=None, flip_y=0.1, class_sep=0.1, random_state=123)\n",
    "database = LabeledDatabase(data, labels)\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = database.load_data()\n",
    "print(\"Shape of train and test data: \" + str(train_data.shape) + str(test_data.shape))\n",
    "print(\"Shape of train and test labels: \" + str(train_labels.shape) + str(test_labels.shape))\n",
    "print(train_data[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the model in a Federated configuration.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid_distribution = shfl.data_distribution.IidDataDistribution(database)\n",
    "federated_data, test_data, test_labels = iid_distribution.get_federated_data(num_nodes=5, percent=100)\n",
    "\n",
    "classes = np.unique(train_labels)\n",
    "def model_builder():\n",
    "    model = LogisticRegressionModel(n_features=n_features, classes=classes, model_inputs={'warm_start':True})\n",
    "    return model\n",
    "\n",
    "aggregator = shfl.federated_aggregator.FedAvgAggregator()\n",
    "\n",
    "federated_government = shfl.federated_government.FederatedGovernment(model_builder, federated_data, aggregator)\n",
    "federated_government.run_rounds(n=2, test_data=test_data, test_label=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison with Centralized training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on centralized data:\n",
    "model_centralized = LogisticRegressionModel(n_features=n_features, classes=classes)\n",
    "model_centralized.train(train_data, train_labels)\n",
    "if n_features == 2:\n",
    "    plot_2D_decision_boundary(model_centralized._model, train_data, train_labels, title = \"Benchmark: Logistic regression using Centralized data\")\n",
    "print('Centralized test performance: ' + str(model_centralized.evaluate(test_data, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Differential Privacy to the Federated model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampling database:\n",
    "n_samples = 500\n",
    "sampling_data, sampling_labels = make_classification(\n",
    "    n_samples=n_samples, n_features=n_features, n_informative=4, \n",
    "    n_redundant=0, n_repeated=0, n_classes=n_classes, \n",
    "    n_clusters_per_class=2, weights=None, flip_y=0.1, class_sep=0.1, random_state=123)   \n",
    "sample_data = np.hstack((sampling_data, sampling_labels.reshape(-1,1)))\n",
    "distribution = UniformDistribution(sample_data)\n",
    "\n",
    "# Sample sensitivity:\n",
    "n_samples = 300\n",
    "max_sensitivity, mean_sensitivity = sampler.sample_sensitivity(\n",
    "    LogisticRegressionSample(n_features=n_features, classes=classes), \n",
    "    L1SensitivityNorm(), distribution, n=n_samples, gamma=0.05)\n",
    "print(\"Max sensitivity from sampling: \" + str(max_sensitivity))\n",
    "print(\"Mean sensitivity from sampling: \" + str(mean_sensitivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shfl.differential_privacy import LaplaceMechanism\n",
    "\n",
    "params_access_definition = LaplaceMechanism(sensitivity=max_sensitivity, epsilon=0.5)\n",
    "federated_governmentDP = shfl.federated_government.FederatedGovernment(\n",
    "    model_builder, federated_data, aggregator, model_params_access=params_access_definition)\n",
    "\n",
    "federated_governmentDP.run_rounds(n=2, test_data=test_data, test_label=test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
